{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW05: Double ML with XGBoost\n",
    "(due October 27th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28534, 21)\n",
      "(13452, 21)\n"
     ]
    }
   ],
   "source": [
    "# Load Data (NLSY)\n",
    "import pandas as pd\n",
    "df = pd.read_stata('http://www.stata-press.com/data/r10/nlswork.dta')\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Y = df['ln_wage']\n",
    "D = df['union']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in X with all predictors that are not colliders\n",
    "X = df[['age', 'year', 'race', 'msp','nev_mar', 'collgrad', 'not_smsa', 'c_city', 'south', 'ind_code', 'ttl_exp' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split into sample A and sample B\n",
    "X_A, X_B, Y_A, Y_B, D_A, D_B = train_test_split(X, Y, D, test_size=0.50, random_state=0)\n",
    "\n",
    "# Within each sample, make a validation set for xgboost early stopping\n",
    "X_A_train, X_A_val, Y_A_train, Y_A_val, D_A_train, D_A_val = train_test_split(X_A, Y_A, D_A, test_size=0.33, random_state=0)\n",
    "X_B_train, X_B_val, Y_B_train, Y_B_val, D_B_train, D_B_val = train_test_split(X_B, Y_B, D_B, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:07:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1598185621802/work/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:07:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1598185621802/work/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:07:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1598185621802/work/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:07:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1598185621802/work/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:linear', random_state=0, reg_alpha=10, reg_lambda=1,\n",
       "             scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1. In both samples, use xgboost regressor to predict log wages (outcome Y)\n",
    "# use early stopping.\n",
    "from xgboost import XGBRegressor\n",
    "xgbr_Y_A = XGBRegressor(objective ='reg:linear', \n",
    "                          colsample_bytree = 0.3, \n",
    "                          learning_rate = 0.1,\n",
    "                          max_depth = 5, \n",
    "                          alpha = 10, \n",
    "                          n_estimators = 100)\n",
    "\n",
    "xgbr_Y_A.fit(X_A_train, Y_A_train, eval_metric=\"rmse\", eval_set=[(X_A_val, Y_A_val)], verbose=False)\n",
    "\n",
    "xgbr_Y_B = XGBRegressor(objective ='reg:linear', \n",
    "                          colsample_bytree = 0.3, \n",
    "                          learning_rate = 0.1,\n",
    "                          max_depth = 5, \n",
    "                          alpha = 10, \n",
    "                          n_estimators = 100)\n",
    "\n",
    "xgbr_Y_B.fit(X_B_train, Y_B_train, eval_metric=\"rmse\", eval_set=[(X_B_val, Y_B_val)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=10,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2. In both samples, use xgboost classifier to predict union status (treatment D)\n",
    "# use early stopping.\n",
    "from xgboost import XGBClassifier\n",
    "xgbc_D_A = XGBClassifier(objective ='binary:logistic', \n",
    "                          colsample_bytree = 0.3, \n",
    "                          learning_rate = 0.1,\n",
    "                          max_depth = 5, \n",
    "                          alpha = 10, \n",
    "                          n_estimators = 100)\n",
    "\n",
    "xgbc_D_A.fit(X_A_train, D_A_train, eval_metric=\"error\", eval_set=[(X_A_val, D_A_val)], verbose=False)\n",
    "\n",
    "xgbc_D_B = XGBClassifier(objective ='binary:logistic', \n",
    "                          colsample_bytree = 0.3, \n",
    "                          learning_rate = 0.1,\n",
    "                          max_depth = 5, \n",
    "                          alpha = 10, \n",
    "                          n_estimators = 100)\n",
    "\n",
    "xgbc_D_B.fit(X_B_train, D_B_train, eval_metric=\"error\", eval_set=[(X_B_val, D_B_val)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Cross-fitting: Form predictions in other sample.\n",
    "\n",
    "# predict wages in sample A using model trained in sample B:\n",
    "Y_hat_A = xgbr_Y_B.predict(X_A)\n",
    "\n",
    "# vice versa:\n",
    "Y_hat_B = xgbr_Y_A.predict(X_B)\n",
    "\n",
    "    \n",
    "# predict union status in sample A using model trained in sample B:\n",
    "D_hat_A = xgbc_D_B.predict(X_A)\n",
    "    \n",
    "# vice versa:\n",
    "D_hat_B = xgbc_D_B.predict(X_A)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals for wages and union status.\n",
    "\n",
    "# residualized wages in samples A and B:\n",
    "y_tilde_A =  Y_A - Y_hat_A\n",
    "y_tilde_B =  Y_B - Y_hat_B\n",
    "\n",
    "# residualized union status in samples A and B:\n",
    "D_tilde_A =  D_A - D_hat_A\n",
    "D_tilde_B =  D_B - D_hat_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /home/manu/applications/anaconda3/lib/python3.7/site-packages (0.11.0)\n",
      "Requirement already satisfied: pandas>=0.21 in /home/manu/applications/anaconda3/lib/python3.7/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /home/manu/applications/anaconda3/lib/python3.7/site-packages (from statsmodels) (1.18.5)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/manu/applications/anaconda3/lib/python3.7/site-packages (from statsmodels) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/manu/applications/anaconda3/lib/python3.7/site-packages (from statsmodels) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/manu/applications/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/manu/applications/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2019.3)\n",
      "Requirement already satisfied: six in /home/manu/applications/anaconda3/lib/python3.7/site-packages (from patsy>=0.5->statsmodels) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# as a shortcut, use statsmodels.\n",
    "# for projects (not homework), we would do this in stata.\n",
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DML Coeff: union    0.084783\n",
      "dtype: float64\n",
      "DML S.E.: union    0.00845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 4. Run OLS regressions and produce DML estimate\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "model_A = sm.OLS(y_tilde_A, D_tilde_A)\n",
    "result_A = model_A.fit(cov_type='HC1')\n",
    "beta_A = result_A.params\n",
    "se_A = result_A.bse\n",
    "\n",
    "model_B = sm.OLS(y_tilde_B, D_tilde_B)\n",
    "result_B = model_B.fit(cov_type='HC1')\n",
    "beta_B = result_B.params\n",
    "se_B = result_B.bse\n",
    "\n",
    "beta = .5 * (beta_A + beta_B)\n",
    "se = .5 * (se_A + se_B)\n",
    "\n",
    "print('DML Coeff:',beta)\n",
    "print('DML S.E.:', se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system('jupyter nbconvert --to html HW05.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
